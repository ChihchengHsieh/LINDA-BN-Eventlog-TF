{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd05c622353f32ef24c8d83e5c3e334107c074e82d7c3e8ca52c56b9fc900ce33e6",
   "display_name": "Python 3.8.10 64-bit ('tf_mac': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5c622353f32ef24c8d83e5c3e334107c074e82d7c3e8ca52c56b9fc900ce33e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants():\n",
    "    PAD_VOCAB = '<PAD>'\n",
    "    UNK_VOCAB = '<UNK>'\n",
    "    SOS_VOCAB = '<SOS>'\n",
    "    EOS_VOCAB = '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "class VocabDict:\n",
    "    def __init__(self, vocab_dict) -> None:\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "    def index_to_vocab(self, index: int) -> str:\n",
    "        for k, v in self.vocab_dict.items():\n",
    "            if (v == index):\n",
    "                return k\n",
    "            continue\n",
    "\n",
    "    def vocab_to_index(self, vocab: str) -> int:\n",
    "        return self.vocab_dict[vocab]\n",
    "\n",
    "    def list_of_index_to_vocab(self, list_of_index: List[int]):\n",
    "        return [self.index_to_vocab(i) for i in list_of_index]\n",
    "\n",
    "    def list_of_vocab_to_index(self, list_of_vocab: List[str]):\n",
    "        return [self.vocab_to_index(v) for v in list_of_vocab]\n",
    "\n",
    "    def vocab_size(self) -> int:\n",
    "        '''\n",
    "        Include <START>, <END> and <PAD> tokens. So, if you the actual number of activities,\n",
    "        you have to minus 3.\n",
    "        '''\n",
    "        return len(self.vocab_dict)\n",
    "    \n",
    "    def padding_index(self):\n",
    "        return self.vocab_to_index(Constants.PAD_VOCAB)\n",
    "\n",
    "    def tranform_to_input_data_from_seq_idx_with_caseid(self, seq_list: List[List[int]], caseids: List[str] = None):\n",
    "        '''\n",
    "        Calculate the lengths for reach trace, so we can use padding.\n",
    "        '''\n",
    "        \n",
    "        seq_lens = np.array([len(s)for s in seq_list])\n",
    "        sorted_len_index = np.flip(np.argsort(seq_lens))\n",
    "        sorted_seq_lens = [seq_lens[idx] for idx in sorted_len_index]\n",
    "        sorted_seq_list = [tf.constant(seq_list[idx])\n",
    "                           for idx in sorted_len_index]\n",
    "\n",
    "        if (caseids):\n",
    "            sorted_caseids = [caseids[i] for i in sorted_len_index]\n",
    "        else:\n",
    "            sorted_caseids = None\n",
    "\n",
    "        return sorted_caseids, tf.keras.preprocessing.sequence.pad_sequences(sorted_seq_list, padding='post', value=0), tf.constant(sorted_seq_lens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.backend.in_train_phase(x, alt, training=None)>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./datasets/preprocessed/BPI_Challenge_2012/AOW/df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [ r.to_dict() for idx, r in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_arrays = [np.array(t)for t  in list(df['trace'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-643bf7663217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"caseid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3153\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    127\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 129\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((list(df[\"caseid\"]), trace_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =  iter(ds).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               trace  caseid\n",
       "0  [2, 12, 9, 10, 27, 28, 3, 17, 8, 15, 19, 33, 2...  173688"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>trace</th>\n      <th>caseid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[2, 12, 9, 10, 27, 28, 3, 17, 8, 15, 19, 33, 2...</td>\n      <td>173688</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df[df['caseid'] == str(int(example.numpy()))]  # Find the correspond case id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(list(range(len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(20)\n",
    "ds = ds.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = list(ds.as_numpy_iterator())[0]\n",
    "batch_df = df.iloc[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseids = list(batch_df[\"caseid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'caseids' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c4a1eadd7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcaseids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'caseids' is not defined"
     ]
    }
   ],
   "source": [
    "caseids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parameters import TrainingParameters, EnviromentParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./datasets/preprocessed/BPI_Challenge_2012'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "EnviromentParameters.BPI2020Dataset.preprocessed_foldr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n=================================================\n| Preprocessed data loaded successfully: ./datasets/preprocessed/BPI_Challenge_2012/OW \n=================================================\n"
     ]
    }
   ],
   "source": [
    "train_param = TrainingParameters()\n",
    "xes_dataset = XESDataset(file_path=EnviromentParameters.BPI2020Dataset.file_path,preprocessed_folder_path=EnviromentParameters.BPI2020Dataset.preprocessed_foldr_path,\n",
    "preprocessed_df_type= EnviromentParameters.BPI2020Dataset.preprocessed_df_type,\n",
    "include_types = train_param.bpi2012.BPI2012_include_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.get_index_ds()\n",
    "ds = ds.shuffle(3).batch(32)\n",
    "batch_idx = list(ds.as_numpy_iterator())[0]\n",
    "caseids, input_data, target = xes_dataset.collate_fn(batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32, 101)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "dataset.collate_fn(batch_idx)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_traces = list(batch_df[\"trace\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traces = [ t[:-1] for t in batch_traces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_traces = [t[1:] for t in batch_traces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data_traces = tf.keras.preprocessing.sequence.pad_sequences(data_traces, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9b4635645c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpadded_target_traces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_traces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "padded_target_traces = tf.keras.preprocessing.sequence.pad_sequences(target_traces, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden = 16\n",
    "lstm = tf.keras.models.Sequential(\n",
    "    [tf.keras.layers.LSTM(\n",
    "                lstm_hidden,\n",
    "                return_sequences = True,\n",
    "                return_state=True,\n",
    "                stateful=True\n",
    "    ),\n",
    "    tf.keras.layers.LSTM(\n",
    "                lstm_hidden,\n",
    "                return_sequences = True,\n",
    "                return_state=True,\n",
    "                stateful=True\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_net = tf.keras.models.Sequential(\n",
    "    [\n",
    "         tf.keras.layers.BatchNormalization(),\n",
    "         tf.keras.layers.LeakyReLU(),\n",
    "         tf.keras.layers.Dropout(.2),\n",
    "         tf.keras.layers.Dense(30),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineLSTM(tf.keras.Model):\n",
    "    def __init__(self, vocab: VocabDict,  embedding_dim: int, lstm_hidden: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(\n",
    "            input_dim = len(vocab),\n",
    "            output_dim = embedding_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        self.lstm =  tf.keras.layers.LSTM(\n",
    "                lstm_hidden,\n",
    "                return_sequences = True,\n",
    "                return_state=True,\n",
    "                stateful=True\n",
    "        )\n",
    "        self.out_net = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.LeakyReLU(),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(len(vocab)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs, state = None):\n",
    "        if not state is None:\n",
    "            self.lstm.reset_states(state)\n",
    "\n",
    "        out = self.emb(inputs)\n",
    "        out, h_out, c_out = self.lstm(out)\n",
    "        out = self.out_net(out)\n",
    "        out = tf.nn.softmax(out, axis= -1)\n",
    "\n",
    "        return out, (h_out, c_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lstm = BaselineLSTM(\n",
    "    xes_dataset.vocab, \n",
    "    embedding_dim= train_param.baselineLSTMModelParameters.embedding_dim,\n",
    "    lstm_hidden= train_param.baselineLSTMModelParameters.lstm_hidden,\n",
    "    dropout=train_param.baselineLSTMModelParameters.dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 101, 29), dtype=float32, numpy=\n",
       " array([[[0.0344136 , 0.03450113, 0.03452059, ..., 0.03445655,\n",
       "          0.0345066 , 0.03448131],\n",
       "         [0.03446138, 0.03447157, 0.03440817, ..., 0.03442244,\n",
       "          0.03440362, 0.03454718],\n",
       "         [0.03454544, 0.0343913 , 0.03460056, ..., 0.03450439,\n",
       "          0.03428044, 0.03433595],\n",
       "         ...,\n",
       "         [0.03484194, 0.03413647, 0.03464504, ..., 0.0344089 ,\n",
       "          0.03451387, 0.03425217],\n",
       "         [0.03484194, 0.03413647, 0.03464504, ..., 0.0344089 ,\n",
       "          0.03451387, 0.03425217],\n",
       "         [0.03484194, 0.03413647, 0.03464504, ..., 0.0344089 ,\n",
       "          0.03451387, 0.03425217]],\n",
       " \n",
       "        [[0.03441359, 0.03450113, 0.03452059, ..., 0.03445655,\n",
       "          0.03450659, 0.0344813 ],\n",
       "         [0.03446138, 0.03447157, 0.03440817, ..., 0.03442244,\n",
       "          0.03440362, 0.03454718],\n",
       "         [0.03454544, 0.0343913 , 0.03460056, ..., 0.03450439,\n",
       "          0.03428044, 0.03433595],\n",
       "         ...,\n",
       "         [0.03450479, 0.03442441, 0.03468112, ..., 0.03465446,\n",
       "          0.03440962, 0.03420024],\n",
       "         [0.03450479, 0.03442441, 0.03468112, ..., 0.03465446,\n",
       "          0.03440962, 0.03420025],\n",
       "         [0.03450479, 0.03442441, 0.03468112, ..., 0.03465446,\n",
       "          0.03440962, 0.03420025]],\n",
       " \n",
       "        [[0.03441359, 0.03450113, 0.03452059, ..., 0.03445655,\n",
       "          0.03450659, 0.0344813 ],\n",
       "         [0.03446138, 0.03447157, 0.03440817, ..., 0.03442244,\n",
       "          0.03440362, 0.03454718],\n",
       "         [0.03454544, 0.0343913 , 0.03460056, ..., 0.03450439,\n",
       "          0.03428043, 0.03433595],\n",
       "         ...,\n",
       "         [0.03456081, 0.03455753, 0.03453349, ..., 0.03428767,\n",
       "          0.03430779, 0.03430868],\n",
       "         [0.0345608 , 0.03455753, 0.03453349, ..., 0.03428766,\n",
       "          0.03430778, 0.03430867],\n",
       "         [0.0345608 , 0.03455753, 0.03453349, ..., 0.03428766,\n",
       "          0.03430778, 0.03430867]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.03441359, 0.03450113, 0.03452059, ..., 0.03445655,\n",
       "          0.03450659, 0.0344813 ],\n",
       "         [0.03446138, 0.03447157, 0.03440817, ..., 0.03442244,\n",
       "          0.03440362, 0.03454718],\n",
       "         [0.03454544, 0.0343913 , 0.03460056, ..., 0.03450439,\n",
       "          0.03428044, 0.03433595],\n",
       "         ...,\n",
       "         [0.03450904, 0.03434874, 0.0347723 , ..., 0.03467829,\n",
       "          0.03441975, 0.03412682],\n",
       "         [0.03450904, 0.03434873, 0.0347723 , ..., 0.03467828,\n",
       "          0.03441975, 0.03412682],\n",
       "         [0.03450904, 0.03434874, 0.0347723 , ..., 0.03467829,\n",
       "          0.03441975, 0.03412682]],\n",
       " \n",
       "        [[0.03441359, 0.03450113, 0.03452059, ..., 0.03445655,\n",
       "          0.03450659, 0.0344813 ],\n",
       "         [0.03446138, 0.03447157, 0.03440817, ..., 0.03442244,\n",
       "          0.03440362, 0.03454718],\n",
       "         [0.03454544, 0.0343913 , 0.03460056, ..., 0.03450439,\n",
       "          0.03428043, 0.03433595],\n",
       "         ...,\n",
       "         [0.03455026, 0.03461864, 0.03456508, ..., 0.0343329 ,\n",
       "          0.03424177, 0.03428372],\n",
       "         [0.03455026, 0.03461864, 0.03456508, ..., 0.0343329 ,\n",
       "          0.03424177, 0.03428371],\n",
       "         [0.03455026, 0.03461864, 0.03456508, ..., 0.0343329 ,\n",
       "          0.03424177, 0.03428372]],\n",
       " \n",
       "        [[0.03441359, 0.03450113, 0.03452059, ..., 0.03445655,\n",
       "          0.03450659, 0.0344813 ],\n",
       "         [0.03446138, 0.03447157, 0.03440817, ..., 0.03442244,\n",
       "          0.03440362, 0.03454718],\n",
       "         [0.03454544, 0.0343913 , 0.03460056, ..., 0.03450439,\n",
       "          0.03428044, 0.03433595],\n",
       "         ...,\n",
       "         [0.03475544, 0.03419022, 0.03431003, ..., 0.03475563,\n",
       "          0.03464283, 0.03451723],\n",
       "         [0.03475544, 0.03419022, 0.03431003, ..., 0.03475563,\n",
       "          0.03464283, 0.03451723],\n",
       "         [0.03475544, 0.03419022, 0.03431003, ..., 0.03475563,\n",
       "          0.03464283, 0.03451723]]], dtype=float32)>,\n",
       " (<tf.Tensor: shape=(32, 64), dtype=float32, numpy=\n",
       "  array([[-0.00172057, -0.0058577 ,  0.0050215 , ...,  0.00342642,\n",
       "           0.00223944,  0.00742209],\n",
       "         [-0.01558166, -0.01066251, -0.00059144, ..., -0.00358572,\n",
       "          -0.01179411,  0.00853423],\n",
       "         [-0.00196359, -0.01774401,  0.00345746, ...,  0.01334447,\n",
       "           0.00157857,  0.00306537],\n",
       "         ...,\n",
       "         [-0.01865668, -0.01185505, -0.0023655 , ..., -0.00661882,\n",
       "          -0.01464162,  0.01158092],\n",
       "         [-0.00236609, -0.01518809,  0.00081423, ...,  0.0132028 ,\n",
       "           0.00241148,  0.00310297],\n",
       "         [ 0.00407074, -0.00138682,  0.00186485, ...,  0.00301303,\n",
       "          -0.01158257,  0.00433121]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(32, 64), dtype=float32, numpy=\n",
       "  array([[-0.00345887, -0.01172808,  0.01008878, ...,  0.00681612,\n",
       "           0.00446071,  0.01498443],\n",
       "         [-0.03139438, -0.02133713, -0.00117172, ..., -0.00728094,\n",
       "          -0.02330906,  0.01714277],\n",
       "         [-0.00388403, -0.03578282,  0.0068787 , ...,  0.02688451,\n",
       "           0.00312416,  0.0061664 ],\n",
       "         ...,\n",
       "         [-0.03762797, -0.02370035, -0.00468944, ..., -0.01343423,\n",
       "          -0.02894496,  0.02327788],\n",
       "         [-0.00467496, -0.03061968,  0.00161943, ...,  0.02659554,\n",
       "           0.00477254,  0.00623519],\n",
       "         [ 0.00819901, -0.00275543,  0.00373072, ...,  0.00601637,\n",
       "          -0.02312819,  0.00864973]], dtype=float32)>))"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "baseline_lstm(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.BaselineLSTM at 0x2957ed220>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = tf.keras.layers.Embedding(\n",
    "            input_dim = vocab_len,\n",
    "            output_dim = 32,\n",
    "            mask_zero=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.layers.LSTM(\n",
    "                64,\n",
    "                return_sequences = True,\n",
    "                return_state=True,\n",
    "                stateful=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = padded_data_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_out = emb(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = emb.compute_mask(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, h_, c_ = lstm(emb_out, mask= mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.reset_states((h_, c_))"
   ]
  },
  {
   "source": [
    "input_data.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 200,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8, 60)"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([8, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "lstm.states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = tf.keras.layers.BatchNormalization()\n",
    "re = tf.keras.layers.LeakyReLU()\n",
    "drop = tf.keras.layers.Dropout(.2)\n",
    "dense = tf.keras.layers.Dense(vocab_len)\n",
    "softmax_out = tf.nn.softmax(dense_out, axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_out = bn(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = tf.keras.layers.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_out = re(bn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = tf.keras.layers.Dropout(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_out = drop(re_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_out = dense(drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_out = tf.nn.softmax(dense_out, axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cce(softmax_out, tf.one_hot(padded_target_traces, depth=vocab_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cce_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=padded_target_traces, y_pred=softmax_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.7052965>"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 60), dtype=float32, numpy=\n",
       "array([[3.6641452, 3.6601377, 3.6568236, 3.6619198, 3.6659055, 3.659903 ,\n",
       "        3.6613874, 3.6573622, 3.6578019, 3.662001 , 3.665196 , 3.6573966,\n",
       "        3.6685224, 3.6639829, 3.6655066, 3.6655397, 3.6615639, 3.6660523,\n",
       "        3.6567807, 3.6647983, 3.6568944, 3.6568944, 3.6568944, 3.6568942,\n",
       "        3.6568942, 3.6568944, 3.6568942, 3.6568942, 3.6568944, 3.6568944,\n",
       "        3.6568944, 3.6568942, 3.6568942, 3.6568944, 3.6568942, 3.6568942,\n",
       "        3.6568944, 3.6568944, 3.6568944, 3.6568942, 3.6568942, 3.6568944,\n",
       "        3.6568942, 3.6568942, 3.6568944, 3.6568944, 3.6568944, 3.6568942,\n",
       "        3.6568942, 3.6568944, 3.6568942, 3.6568942, 3.6568944, 3.6568944,\n",
       "        3.6568944, 3.6568942, 3.6568942, 3.6568944, 3.6568942, 3.6568942],\n",
       "       [3.6641452, 3.6601377, 3.6568236, 3.6619196, 3.6659055, 3.6599033,\n",
       "        3.6613874, 3.6573622, 3.6643739, 3.6623595, 3.6692684, 3.666542 ,\n",
       "        3.662608 , 3.6712987, 3.6649058, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618,\n",
       "        3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618, 3.6730618],\n",
       "       [3.6641452, 3.6601377, 3.6659489, 3.6558642, 3.6546443, 3.6586812,\n",
       "        3.6630113, 3.6656737, 3.6568756, 3.6688085, 3.664367 , 3.6655824,\n",
       "        3.6664674, 3.6555872, 3.663631 , 3.6554644, 3.6612115, 3.6542118,\n",
       "        3.6591825, 3.6654272, 3.6670992, 3.6554358, 3.6658604, 3.659153 ,\n",
       "        3.660287 , 3.6692333, 3.6626387, 3.6597838, 3.6635184, 3.6538038,\n",
       "        3.6641917, 3.6501381, 3.6622374, 3.6609354, 3.6634145, 3.6614633,\n",
       "        3.66382  , 3.66382  , 3.66382  , 3.66382  , 3.66382  , 3.6638203,\n",
       "        3.6638203, 3.6638203, 3.66382  , 3.66382  , 3.66382  , 3.66382  ,\n",
       "        3.66382  , 3.6638203, 3.6638203, 3.6638203, 3.66382  , 3.66382  ,\n",
       "        3.66382  , 3.66382  , 3.66382  , 3.6638203, 3.6638203, 3.6638203],\n",
       "       [3.6641452, 3.6601377, 3.6659489, 3.655864 , 3.6546443, 3.6673772,\n",
       "        3.6609452, 3.6608872, 3.656345 , 3.6603618, 3.6559515, 3.6745539,\n",
       "        3.6667309, 3.6669884, 3.6675   , 3.6574142, 3.6644893, 3.6679554,\n",
       "        3.6638215, 3.6614547, 3.664448 , 3.6630406, 3.6609542, 3.6572971,\n",
       "        3.6568701, 3.6592803, 3.6592803, 3.6592803, 3.6592805, 3.6592803,\n",
       "        3.6592803, 3.6592803, 3.6592803, 3.6592803, 3.6592803, 3.6592803,\n",
       "        3.6592805, 3.6592803, 3.6592803, 3.6592803, 3.6592803, 3.6592803,\n",
       "        3.6592803, 3.6592803, 3.6592805, 3.6592803, 3.6592803, 3.6592803,\n",
       "        3.6592803, 3.6592803, 3.6592803, 3.6592803, 3.6592805, 3.6592803,\n",
       "        3.6592803, 3.6592803, 3.6592803, 3.6592803, 3.6592803, 3.6592803],\n",
       "       [3.6641452, 3.6601377, 3.6659489, 3.6558642, 3.6546443, 3.6673772,\n",
       "        3.6609454, 3.6715653, 3.6663935, 3.6743963, 3.6693015, 3.6622214,\n",
       "        3.652256 , 3.654609 , 3.6536033, 3.6805258, 3.6705182, 3.669186 ,\n",
       "        3.6683018, 3.6597693, 3.6597192, 3.6607425, 3.6708016, 3.6658008,\n",
       "        3.6556392, 3.6587727, 3.6550612, 3.6580493, 3.6536436, 3.6570044,\n",
       "        3.6519935, 3.6559281, 3.6758919, 3.6483977, 3.6671295, 3.6549978,\n",
       "        3.6559231, 3.6460187, 3.654188 , 3.6476784, 3.6551957, 3.647756 ,\n",
       "        3.6550922, 3.6472387, 3.6547797, 3.6464782, 3.6544929, 3.6456513,\n",
       "        3.6542892, 3.6662743, 3.6677098, 3.6486852, 3.6675446, 3.6726644,\n",
       "        3.6623478, 3.6724262, 3.6704319, 3.665655 , 3.6591392, 3.6637166],\n",
       "       [3.6641452, 3.6601377, 3.6659489, 3.655864 , 3.6546443, 3.6673772,\n",
       "        3.6609452, 3.6715653, 3.6663935, 3.6743963, 3.6759272, 3.6512964,\n",
       "        3.6714222, 3.6724   , 3.6723998, 3.6723998, 3.6724   , 3.6724   ,\n",
       "        3.6723998, 3.6723998, 3.6723998, 3.6724   , 3.6723998, 3.6723998,\n",
       "        3.6724   , 3.6724   , 3.6723998, 3.6723998, 3.6723998, 3.6724   ,\n",
       "        3.6723998, 3.6723998, 3.6724   , 3.6724   , 3.6723998, 3.6723998,\n",
       "        3.6723998, 3.6724   , 3.6723998, 3.6723998, 3.6724   , 3.6724   ,\n",
       "        3.6723998, 3.6723998, 3.6723998, 3.6724   , 3.6723998, 3.6723998,\n",
       "        3.6724   , 3.6724   , 3.6723998, 3.6723998, 3.6723998, 3.6724   ,\n",
       "        3.6723998, 3.6723998, 3.6724   , 3.6724   , 3.6723998, 3.6723998],\n",
       "       [3.6641452, 3.6601377, 3.6698358, 3.6608582, 3.663941 , 3.663941 ,\n",
       "        3.6639411, 3.6639411, 3.6639411, 3.663941 , 3.663941 , 3.663941 ,\n",
       "        3.663941 , 3.663941 , 3.6639411, 3.6639411, 3.6639411, 3.663941 ,\n",
       "        3.663941 , 3.663941 , 3.663941 , 3.663941 , 3.6639411, 3.6639411,\n",
       "        3.6639411, 3.663941 , 3.663941 , 3.663941 , 3.663941 , 3.663941 ,\n",
       "        3.6639411, 3.6639411, 3.6639411, 3.663941 , 3.663941 , 3.663941 ,\n",
       "        3.663941 , 3.663941 , 3.6639411, 3.6639411, 3.6639411, 3.663941 ,\n",
       "        3.663941 , 3.663941 , 3.663941 , 3.663941 , 3.6639411, 3.6639411,\n",
       "        3.6639411, 3.663941 , 3.663941 , 3.663941 , 3.663941 , 3.663941 ,\n",
       "        3.6639411, 3.6639411, 3.6639411, 3.663941 , 3.663941 , 3.663941 ],\n",
       "       [3.6641452, 3.6601377, 3.6568236, 3.6619196, 3.6602104, 3.6591413,\n",
       "        3.6645525, 3.659258 , 3.6602902, 3.6564605, 3.664503 , 3.6614528,\n",
       "        3.6692755, 3.665666 , 3.6731791, 3.6686602, 3.6761472, 3.6703002,\n",
       "        3.6591518, 3.676739 , 3.6655705, 3.6749425, 3.6749425, 3.6749425,\n",
       "        3.6749427, 3.6749427, 3.6749425, 3.6749427, 3.6749425, 3.6749425,\n",
       "        3.6749425, 3.6749425, 3.6749427, 3.6749427, 3.6749425, 3.6749427,\n",
       "        3.6749425, 3.6749425, 3.6749425, 3.6749425, 3.6749427, 3.6749427,\n",
       "        3.6749425, 3.6749427, 3.6749425, 3.6749425, 3.6749425, 3.6749425,\n",
       "        3.6749427, 3.6749427, 3.6749425, 3.6749427, 3.6749425, 3.6749425,\n",
       "        3.6749425, 3.6749425, 3.6749427, 3.6749427, 3.6749425, 3.6749427]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "sparse_cce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.664769>"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "source": [
    "tf.reduce_mean(sparse_cce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "type(list(df[\"caseid\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m           \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0m\u001b[1;32m    480\u001b[0m                   (element, type(element).__name__))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for [[1.3], [3, 4, 6], [1]] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a7c5e03b197a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"caseid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trace\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3153\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_mac/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "tf.data.Dataset.from_tensor_slices({\"caseid\": [1,2,3], \"trace\": [[1.3],[3,4,6],[1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.PrintUtils import print_big\n",
    "from CustomExceptions import NotSupportedError\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import pm4py\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from Utils.Constants import Constants\n",
    "from Utils.FileUtils import file_exists\n",
    "import json\n",
    "import os\n",
    "from Parameters.Enums import PreprocessedDfType, ActivityType\n",
    "\n",
    "class XESDataset(Dataset):\n",
    "    pickle_df_file_name = \"df.pickle\"\n",
    "    vocab_dict_file_name = \"vocab_dict.json\"\n",
    "\n",
    "    def __init__(self, file_path: str, preprocessed_folder_path: str, preprocessed_df_type: PreprocessedDfType, device: torch.device = torch.device(\"cpu\"), include_types: list[ActivityType] = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.file_path = file_path\n",
    "        self.preprocessed_folder_path = os.path.join(\n",
    "            preprocessed_folder_path, XESDataset.get_type_folder_name(include_types))\n",
    "        self.preprocessed_df_type = preprocessed_df_type\n",
    "\n",
    "        if (not preprocessed_folder_path is None) and self.preprocessed_data_exist(self.preprocessed_folder_path, self.preprocessed_df_type):\n",
    "            self.load_preprocessed_data()\n",
    "        else:\n",
    "            self.__initialise_data(\n",
    "                file_path=file_path, include_types=include_types)\n",
    "\n",
    "            if not preprocessed_folder_path is None:\n",
    "                self.save_preprocessed_data()\n",
    "\n",
    "    def __initialise_data(self, file_path: str, include_types: list[ActivityType]) -> None:\n",
    "        '''\n",
    "        run this function if the preprocessed data doesn't exist.\n",
    "        [file_path]: path of `BPI_Challenge_2012.xes` \n",
    "        [include_types]: what types of activity you want to load.\n",
    "        '''\n",
    "        ############ load xes file and extract needed information ############\n",
    "        log = pm4py.read_xes(file_path)\n",
    "        flattern_log: list[dict[str, any]] = ([{**event,\n",
    "                                                'caseid': trace.attributes['concept:name']}\n",
    "                                               for trace in log for event in trace])\n",
    "        df = pd.DataFrame(flattern_log)\n",
    "\n",
    "        if not (include_types is None):\n",
    "            df = df[[any(bool_set) for bool_set in zip(\n",
    "                *([df[\"concept:name\"].str.startswith(a.value) for a in include_types]))]]\n",
    "\n",
    "        df[\"name_and_transition\"] = df[\"concept:name\"] + \\\n",
    "            \"_\" + df[\"lifecycle:transition\"]\n",
    "        df = df[['time:timestamp', 'name_and_transition', \"caseid\"]]\n",
    "\n",
    "        ############ Append starting and ending time for each trace ############\n",
    "        newData = list() \n",
    "        for case, group in df.groupby('caseid'):\n",
    "            group.sort_values(\"time:timestamp\", ascending=True, inplace=True)\n",
    "            strating_time = group.iloc[0][\"time:timestamp\"] - \\\n",
    "                timedelta(microseconds=1)\n",
    "            ending_time = group.iloc[-1][\"time:timestamp\"] + \\\n",
    "                timedelta(microseconds=1)\n",
    "            traces = group.to_dict('records')\n",
    "\n",
    "            # Add start and end tags.\n",
    "            traces.insert(\n",
    "                0, {\"caseid\": case, \"time:timestamp\": strating_time, \"name_and_transition\": Constants.SOS_VOCAB})\n",
    "            traces.append(\n",
    "                {\"caseid\": case, \"time:timestamp\": ending_time, \"name_and_transition\": Constants.EOS_VOCAB})\n",
    "            newData.extend(traces)\n",
    "\n",
    "        df = pd.DataFrame(newData)\n",
    "        df['name_and_transition'] = df['name_and_transition'].astype(\n",
    "            'category')\n",
    "\n",
    "        ############ generate vocabulary dictionary ############\n",
    "        vocab_dict: dict[str, int] = {}\n",
    "        for i, cat in enumerate(df['name_and_transition'].cat.categories):\n",
    "            # plus one, since we want to remain \"0\" for \"<PAD>\"\n",
    "            vocab_dict[cat] = i+1\n",
    "        vocab_dict[Constants.PAD_VOCAB] = 0\n",
    "\n",
    "        ############ Create new index categorial column ############\n",
    "        df['cat'] = df['name_and_transition'].apply(lambda c: vocab_dict[c])\n",
    "\n",
    "        ############ Create the df only consisted of trace and caseid ############\n",
    "        final_df_data: list[dict[str, any]] = []\n",
    "        for caseid, group in df.groupby('caseid'):\n",
    "            final_df_data.append({\n",
    "                \"trace\": list(group['cat']),\n",
    "                \"caseid\": caseid\n",
    "            })\n",
    "\n",
    "        ############ store data in instance ############    \n",
    "        self.df: pd.DataFrame = pd.DataFrame(final_df_data)\n",
    "        self.df.sort_values(\"caseid\", inplace=True)\n",
    "        self.vocab = VocabDict(vocab_dict)\n",
    "\n",
    "    def longest_trace_len(self) -> int:\n",
    "        return self.df.trace.map(len).max()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index: int) -> pd.Series:\n",
    "        return self.df.iloc[index]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type_folder_name(include_types: list[ActivityType] = None):\n",
    "        if include_types is None:\n",
    "            return \"All\"\n",
    "\n",
    "        return \"\".join(\n",
    "            sorted([a.value for a in include_types], key=str.lower))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_name_from_preprocessed_df_type(preprocessed_df_type: PreprocessedDfType):\n",
    "        if preprocessed_df_type == PreprocessedDfType.Pickle:\n",
    "            return XESDataset.pickle_df_file_name\n",
    "        else:\n",
    "            raise NotSupportedError(\n",
    "                \"Not supported saving format for preprocessed data\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocessed_data_exist(preprocessed_folder_path: str, preprocessed_df_type: PreprocessedDfType, ):\n",
    "        file_name = XESDataset.get_file_name_from_preprocessed_df_type(\n",
    "            preprocessed_df_type)\n",
    "        df_path = os.path.join(preprocessed_folder_path,  file_name)\n",
    "        vocab_dict_path = os.path.join(\n",
    "            preprocessed_folder_path, XESDataset.vocab_dict_file_name)\n",
    "        return file_exists(df_path) and file_exists(vocab_dict_path)\n",
    "\n",
    "    def store_df(self, preprocessed_folder_path: str, preprocessed_df_type: PreprocessedDfType):\n",
    "        os.makedirs(preprocessed_folder_path, exist_ok=True)\n",
    "        file_name = XESDataset.get_file_name_from_preprocessed_df_type(\n",
    "            preprocessed_df_type)\n",
    "        df_path = os.path.join(preprocessed_folder_path, file_name)\n",
    "        if(preprocessed_df_type == PreprocessedDfType.Pickle):\n",
    "            self.store_df_in_pickle(df_path)\n",
    "        else:\n",
    "            raise NotSupportedError(\n",
    "                \"Not supported saving format for preprocessed data\")\n",
    "\n",
    "    def load_df(self, preprocessed_folder_path: str, preprocessed_df_type: PreprocessedDfType):\n",
    "        file_name = XESDataset.get_file_name_from_preprocessed_df_type(\n",
    "            preprocessed_df_type)\n",
    "        df_path = os.path.join(preprocessed_folder_path, file_name)\n",
    "        if(preprocessed_df_type == PreprocessedDfType.Pickle):\n",
    "            self.load_df_from_pickle(df_path)\n",
    "        else:\n",
    "            raise NotSupportedError(\n",
    "                \"Not supported loading format for preprocessed data\")\n",
    "\n",
    "    def store_df_in_pickle(self, path):\n",
    "        self.df.to_pickle(path)\n",
    "\n",
    "    def load_df_from_pickle(self, path):\n",
    "        self.df = pd.read_pickle(path)\n",
    "\n",
    "    def save_preprocessed_data(self):\n",
    "        if self.preprocessed_folder_path is None:\n",
    "            raise Error(\"Preprocessed folder path can't be None\")\n",
    "\n",
    "        ############ Store df ############\n",
    "        self.store_df(self.preprocessed_folder_path,\n",
    "                      self.preprocessed_df_type)\n",
    "\n",
    "        ############ Store vocab_dict ############\n",
    "        vocab_dict_path = os.path.join(\n",
    "            self.preprocessed_folder_path, XESDataset.vocab_dict_file_name)\n",
    "        with open(vocab_dict_path, 'w') as output_file:\n",
    "            json.dump(self.vocab.vocab_dict, output_file, indent='\\t')\n",
    "\n",
    "        print_big(\n",
    "            \"Preprocessed data saved successfully\"\n",
    "        )\n",
    "\n",
    "    def load_preprocessed_data(self):\n",
    "        if self.preprocessed_folder_path is None:\n",
    "            raise Error(\"Preprocessed folder path can't be None\")\n",
    "\n",
    "        ############ Load df ############\n",
    "        self.load_df(self.preprocessed_folder_path, self.preprocessed_df_type)\n",
    "\n",
    "        ############ load vocab_dict ############\n",
    "        vocab_dict_path = os.path.join(\n",
    "            self.preprocessed_folder_path, XESDataset.vocab_dict_file_name)\n",
    "        with open(vocab_dict_path, 'r') as output_file:\n",
    "            vocab_dict = json.load(output_file)\n",
    "            self.vocab = VocabDict(vocab_dict)\n",
    "\n",
    "        print_big(\n",
    "            \"Preprocessed data loaded successfully: %s\" % (self.preprocessed_folder_path)\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, data: list[pd.Series]) -> tuple[np.ndarray, torch.Tensor, torch.Tensor, np.ndarray]:\n",
    "        caseid_list, seq_list = zip(\n",
    "            *[(d[\"caseid\"], torch.tensor(d[\"trace\"])) for d in data])\n",
    "        caseid_list = list(caseid_list)\n",
    "        seq_list = list(seq_list)\n",
    "\n",
    "        ############ Get sorting index ############\n",
    "        seq_lens_before_splitting = np.array([len(s)for s in seq_list])\n",
    "        sorted_len_index = np.flip(np.argsort(seq_lens_before_splitting))\n",
    "\n",
    "        ############ Sort caseids and traces ############\n",
    "        sorted_seq_list = [seq_list[idx]\n",
    "                           for idx in sorted_len_index]\n",
    "        sorted_case_id = np.array(caseid_list)[sorted_len_index]\n",
    "\n",
    "        ############ Build training and test seq ############\n",
    "        # this should remove all the EOS to form a training set\n",
    "        data_seq_list = [li[:-1] for li in sorted_seq_list]\n",
    "        \n",
    "        # this should remove all the SOS to form a testing set\n",
    "        target_seq_list = [li[1:] for li in sorted_seq_list]\n",
    "\n",
    "        ############ Get lengths ############\n",
    "        data_seq_length = [len(l) for l in data_seq_list]\n",
    "\n",
    "        ############ Pad input and target ############\n",
    "        padded_data = pad_sequence(\n",
    "            data_seq_list, batch_first=True,  padding_value=0)\n",
    "        padded_target = pad_sequence(\n",
    "            target_seq_list, batch_first=True, padding_value=0)\n",
    "\n",
    "        return sorted_case_id, padded_data.to(self.device), torch.tensor(data_seq_length).to(self.device),padded_target.to(self.device)\n",
    "\n",
    "    def get_sampler_from_df(self, df, seed):\n",
    "        return None\n",
    "\n",
    "    def get_train_shuffle(self):\n",
    "        return True\n"
   ]
  }
 ]
}