{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf_mac': conda)"
  },
  "interpreter": {
   "hash": "5c622353f32ef24c8d83e5c3e334107c074e82d7c3e8ca52c56b9fc900ce33e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.BaselineLSTMWithResourceValidTraceCf import BaselineLSTMWithResourceValidTraceCf\n",
    "from Utils.Preprocessing import dataset_split\n",
    "from Models import BaseNN, BaselineLSTMWithResource\n",
    "from Data.MedicalDataset import MedicalDataset\n",
    "from typing import List, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from Models.BaselineLSTM import BaselineLSTM\n",
    "from CustomExceptions.Exceptions import NotSupportedError\n",
    "from Parameters.EnviromentParameters import EnviromentParameters\n",
    "from Parameters.Enums import SelectableDatasets, SelectableLoss, SelectableModels, SelectableOptimizer\n",
    "from Parameters import TrainingParameters\n",
    "from Utils.PrintUtils import print_big, print_peforming_task\n",
    "from Data import BPI2012ValidTraceDataset, XESDataset, XESDatasetWithResource\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "from Utils.SaveUtils import save_parameters_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Controller import TraceClassifierController\n",
    "from Utils.SaveUtils import load_parameters\n",
    "from Parameters import TrainingParameters\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n========================================\n| Preprocessed data loaded successfully: ./datasets/preprocessed/BPI_Challenge_2012_valid_trace/All \n========================================\n"
     ]
    }
   ],
   "source": [
    "### Init dataset \n",
    "dataset = BPI2012ValidTraceDataset(\n",
    "file_path=EnviromentParameters.BPI2012ValidTraceDataset.file_path,\n",
    "preprocessed_folder_path=EnviromentParameters.BPI2012ValidTraceDataset.preprocessed_foldr_path,\n",
    "preprocessed_df_type=EnviromentParameters.BPI2012ValidTraceDataset.preprocessed_df_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = TrainingParameters()\n",
    "tf.random.set_seed(parameters.dataset_split_seed)\n",
    "np.random.seed(parameters.dataset_split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, validation_dataset = dataset_split(list(range(len(\n",
    "            dataset))), parameters.train_test_split_portion, seed=parameters.dataset_split_seed,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    train_dataset).batch(parameters.batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    test_dataset).batch(parameters.batch_size)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    validation_dataset).batch(parameters.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(\n",
    "                learning_rate=parameters.optimizerParameters.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "parameters.optimizerParameters.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = (list(train_dataset.as_numpy_iterator()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseids, padded_data_traces, lengths, padded_data_resources, batch_amount, padded_target_traces, real_trace = dataset.collate_fn(batch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'197674'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "caseids[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "lengths[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "real_trace[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 2, 12,  9, 10, 22,  3,  8, 17, 15, 19, 22, 18, 24, 13,  5,  4, 11,\n",
       "       25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "padded_data_traces[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([12,  9, 10, 22,  3,  8, 17, 15, 19, 22, 18, 24, 13,  5,  4, 11, 25,\n",
       "        1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "padded_target_traces[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineLSTMWithResourceValidTraceCf(\n",
    "            activity_vocab=dataset.activity_vocab,\n",
    "            resource_vocab=dataset.resource_vocab,\n",
    "            dense_dim=parameters.baselineLSTMWithResourceparameters.dense_dim,\n",
    "            activity_embedding_dim=parameters.baselineLSTMWithResourceparameters.activity_embedding_dim,\n",
    "            resource_embedding_dim=parameters.baselineLSTMWithResourceparameters.resource_embedding_dim,\n",
    "            lstm_hidden=parameters.baselineLSTMWithResourceparameters.lstm_hidden,\n",
    "            dropout=parameters.baselineLSTMWithResourceparameters.dropout,\n",
    "        )\n",
    "\n",
    "\n",
    "another_model = BaselineLSTMWithResource(\n",
    "                vocab=dataset.activity_vocab,\n",
    "                resources=dataset.resource_vocab.vocabs,\n",
    "                dense_dim=parameters.baselineLSTMWithResourceparameters.dense_dim,\n",
    "                activity_embedding_dim=parameters.baselineLSTMWithResourceparameters.activity_embedding_dim,\n",
    "                resource_embedding_dim=parameters.baselineLSTMWithResourceparameters.resource_embedding_dim,\n",
    "                lstm_hidden=parameters.baselineLSTMWithResourceparameters.lstm_hidden,\n",
    "                dropout=parameters.baselineLSTMWithResourceparameters.dropout,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    out = another_model(padded_data_traces,\n",
    "        padded_data_resources, batch_amount, training=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tape.gradient(out, another_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Is training: True\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    out = model(padded_data_traces,\n",
    "        padded_data_resources, batch_amount, training=True)\n",
    "    # loss_all = tf.keras.losses.binary_crossentropy(\n",
    "    #             y_true=real_trace[:, :, tf.newaxis], y_pred=out)\n",
    "    # mask = tf.cast(real_trace != -1, dtype=tf.float32)\n",
    "    # loss_all = loss_all * mask\n",
    "    # loss = tf.reduce_mean(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tape.gradient(out, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([64, 96, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 96), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}