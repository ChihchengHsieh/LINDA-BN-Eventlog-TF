{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf_mac': conda)"
  },
  "interpreter": {
   "hash": "5c622353f32ef24c8d83e5c3e334107c074e82d7c3e8ca52c56b9fc900ce33e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parameters import TrainingParameters, EnviromentParameters\n",
    "from Controller import TrainingController\n",
    "from Utils.SaveUtils import load_parameters\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Data import XESDatasetWithResource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./SavedModels/%s\" % (\n",
    "    \"0.8264_BPI2012WithResource_BaselineLSTMWithResource_2021-06-18 06:11:10.009443\" # AOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_json = load_parameters(folder_path=folder_path)\n",
    "parameters = TrainingParameters(**parameters_json)\n",
    "tf.random.set_seed(parameters.dataset_split_seed)\n",
    "np.random.seed(parameters.dataset_split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=================================================\n",
      "| Running on /job:localhost/replica:0/task:0/device:CPU:0  \n",
      "=================================================\n",
      "\n",
      "=================================================\n",
      "| Preprocessed data loaded successfully: ./datasets/preprocessed/BPI_Challenge_2012_with_resource/AOW \n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainingController(parameters = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trace_length = max([ len(t) for t in list(trainer.dataset.df['trace'])])\n",
    "vocab_size = len(trainer.dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare model ####\n",
    "model = Transformer(\n",
    "    num_layers=parameters.transformerParameters.num_layers,\n",
    "    d_model=parameters.transformerParameters.model_dim,\n",
    "    num_heads=parameters.transformerParameters.num_heads,\n",
    "    dff=parameters.transformerParameters.feed_forward_dim,\n",
    "    input_vocab_size=vocab_size,\n",
    "    target_vocab_size=vocab_size,\n",
    "    pe_input= max_trace_length * 10,\n",
    "    pe_target= max_trace_length * 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take a batch from dataset ####\n",
    "batch_index = list(trainer.train_dataset.as_numpy_iterator())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_training_data =  trainer.dataset.collate_fn(batch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseids, padded_data_traces, lengths, padded_data_resources, batch_amount, padded_target_traces =  batch_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.scheduler import CustomSchedule\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "learning_rate = CustomSchedule(parameters.transformerParameters.model_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.masking import create_masks\n",
    "\n",
    "def train_step(inp, tar):\n",
    "#   tar_inp = tar[:, :-1]\n",
    "#   tar_real = tar[:, 1:]\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, inp)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = model(inp, inp,\n",
    "                                 True,\n",
    "                                 enc_padding_mask,\n",
    "                                 combined_mask,\n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.utils import loss_function, accuracy_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Step 10 Loss 3.1712 Accuracy 0.1259\n",
      "Epoch 1 Step 20 Loss 3.0832 Accuracy 0.1481\n",
      "Epoch 1 Step 30 Loss 3.0053 Accuracy 0.1686\n",
      "Epoch 1 Step 40 Loss 2.9431 Accuracy 0.1837\n",
      "Epoch 1 Step 50 Loss 2.8798 Accuracy 0.1969\n",
      "Epoch 1 Step 60 Loss 2.8180 Accuracy 0.2091\n",
      "Epoch 1 Step 70 Loss 2.7481 Accuracy 0.2256\n",
      "Epoch 1 Step 80 Loss 2.6746 Accuracy 0.2440\n",
      "Epoch 1 Loss 2.6604 Accuracy 0.2475\n",
      "Time taken for 1 epoch: 125.01 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for train_idxs in trainer.train_dataset:\n",
    "        step += 1\n",
    "        caseids, padded_data_traces, lengths, padded_data_resources, batch_amount, padded_target_traces  = trainer.dataset.collate_fn(train_idxs)\n",
    "\n",
    "        train_step(padded_data_traces, padded_target_traces)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}